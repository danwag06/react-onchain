/**
 * New Wave-Based Orchestrator
 *
 * This orchestrator uses the parallel inscription system with wave-based processing
 * that respects topological dependencies while maximizing parallelism.
 */

import { readFile, writeFile } from 'fs/promises';
import { existsSync } from 'fs';
import { createHash } from 'crypto';
import { PrivateKey } from '@bsv/sdk';
import { analyzeBuildDirectory } from '../analysis/index.js';
import type { FileReference } from '../analysis/index.js';
import { createIndexer, config as envConfig } from '../../lib/config.js';
import { calculateDependencyWaves, prepareWaveJobs, processWaveResults } from './jobBuilder.js';
import { shouldChunkFile } from '../chunking/index.js';
import { parallelInscribe } from '../inscription/index.js';
import { generateChunkReassemblyServiceWorker } from '../service-worker/index.js';
import {
  deployVersioningInscription,
  updateVersioningInscription,
  checkVersionExists,
  VERSIONING_ENABLED,
} from '../versioning/index.js';
import type {
  DeploymentConfig,
  DeploymentResult,
  DeploymentManifest,
  DeploymentManifestHistory,
  ChunkedFileInfo,
  OrchestratorCallbacks,
  WaveJobContext,
} from './orchestration.types.js';
import type { InscribedFile } from '../inscription/index.js';
import type { ChunkManifest } from '../chunking/index.js';
import type { Utxo } from 'js-1sat-ord';
import { formatError } from '../../utils/errors.js';
import {
  MANIFEST_FILENAME,
  VERSIONING_ORIGIN_TYPE,
  VERSIONING_METADATA_TYPE,
  CONTENT_PATH_PREFIX,
  OUTPOINT_SEPARATOR,
  CACHED_FILE_DELIMITER,
  DEFAULT_SATS_PER_KB,
  DEFAULT_INSCRIPTION_VOUT,
  MANIFEST_VERSION,
  MOCK_VERSIONING_TXID,
  DEFAULT_CHUNK_SIZE,
  DEFAULT_CHUNK_THRESHOLD,
  DEFAULT_CHUNK_BATCH_SIZE,
} from '../../utils/constants.js';
import {
  extractOutpointFromFile,
  calculateDependencyHash,
  suggestNextVersion,
  isIndexHtmlFile,
  prepareHtmlContent,
  createInitialHtmlOrdinal,
  updateHtmlOrdinal,
} from '../inscription/index.js';

// ============================================================================
// Helper Functions
// ============================================================================

function initializePaymentKey(paymentKey: string, dryRun: boolean): PrivateKey {
  if (dryRun) {
    return PrivateKey.fromRandom();
  }
  return PrivateKey.fromWif(paymentKey);
}

interface ManifestData {
  existingVersions: string[];
  previousInscriptions: Map<string, InscribedFile>;
  versioningOrigin?: string;
  htmlOrigin?: string;
}

async function loadManifestData(manifestPath: string): Promise<ManifestData | null> {
  if (!existsSync(manifestPath)) {
    return null;
  }

  try {
    const manifestJson = await readFile(manifestPath, 'utf-8');
    const manifestData = JSON.parse(manifestJson);

    const result: ManifestData = {
      existingVersions: [],
      previousInscriptions: new Map(),
    };

    if ('manifestVersion' in manifestData && 'deployments' in manifestData) {
      const history = manifestData as DeploymentManifestHistory;
      result.existingVersions = history.deployments
        .map((d) => d.version)
        .filter((v): v is string => v !== undefined);
      result.versioningOrigin = history.originVersioningInscription;
      result.htmlOrigin = history.originHtmlInscription;

      if (history.deployments.length > 0) {
        const recentDeployment = history.deployments[history.deployments.length - 1];

        // Load all files from manifest (new files with full details)
        for (const file of recentDeployment.files) {
          result.previousInscriptions.set(file.originalPath, file);
        }

        // Load cached files (minimal string references from previous deployments)
        // Format: "path::*::txid_vout"
        // Search backwards through deployment history to find full file details
        if (Array.isArray(recentDeployment.cachedFiles)) {
          for (const cachedStr of recentDeployment.cachedFiles) {
            // Parse the cached file string
            const [originalPath, outpoint] = cachedStr.split(CACHED_FILE_DELIMITER);
            const [txid, voutStr] = outpoint.split('_');
            const vout = parseInt(voutStr, 10);

            // Search previous deployments for this file
            let foundFile: InscribedFile | undefined;
            for (let i = history.deployments.length - 1; i >= 0 && !foundFile; i--) {
              const deployment = history.deployments[i];
              foundFile = deployment.files.find(
                (f) => f.originalPath === originalPath && f.txid === txid && f.vout === vout
              );
            }

            if (foundFile) {
              result.previousInscriptions.set(originalPath, foundFile);
            }
          }
        }
      }
    } else if ('timestamp' in manifestData && 'entryPoint' in manifestData) {
      const manifest = manifestData as DeploymentManifest;
      if (manifest.version) {
        result.existingVersions = [manifest.version];
      }

      // Load all new files from manifest
      for (const file of manifest.files) {
        result.previousInscriptions.set(file.originalPath, file);
      }

      // For backward compatibility: old manifests may have cached files in the files array
      // (identified by the cached flag). These are already loaded above.
    }

    return result;
  } catch {
    return null;
  }
}

function canReuseInscription(
  filePath: string,
  fileRef: FileReference,
  previousInscription: InscribedFile | undefined,
  urlMap: Map<string, string>
): boolean {
  if (!previousInscription) return false;
  if (isIndexHtmlFile(filePath)) return false;

  // Content hash comparison
  if (previousInscription.contentHash) {
    // Previous inscription has content hash - compare directly
    if (previousInscription.contentHash !== fileRef.contentHash) return false;
  } else if (previousInscription.isChunked && previousInscription.chunks) {
    // For chunked files from old deployments without contentHash,
    // we can still reuse if we calculate the hash from stored chunk hashes
    // The fileRef.contentHash is the hash of the entire file content
    // We need to verify this matches the concatenation of previous chunks
    // For now, we'll trust that if chunks exist and file size matches, we can reuse
    // This is safe because chunks contain full file data
    if (previousInscription.size !== fileRef.fileSize) return false;
    // Size matches - assume content is the same (chunks are immutable on chain)
  } else {
    // No contentHash and not chunked (or chunks missing) - can't verify, must re-inscribe
    return false;
  }

  if (fileRef.dependencies.length === 0) return true;

  const currentDepHash = calculateDependencyHash(fileRef.dependencies, urlMap);
  return previousInscription.dependencyHash === currentDepHash;
}

// ============================================================================
// Main Deployment Function
// ============================================================================

export async function deployToChain(
  config: DeploymentConfig,
  callbacks?: OrchestratorCallbacks
): Promise<DeploymentResult> {
  const {
    buildDir,
    paymentKey,
    satsPerKb,
    dryRun,
    version,
    versionDescription,
    versioningOriginInscription,
    appName,
    ordinalContentUrl,
    ordinalIndexerUrl,
  } = config;

  const primaryContentUrl = ordinalContentUrl || envConfig.ordinalContentUrl;
  const paymentPk = initializePaymentKey(paymentKey, dryRun || false);
  const destinationAddress = paymentPk.toAddress().toString();
  const indexer = createIndexer(ordinalIndexerUrl);

  // Step 1: Analyze build directory
  callbacks?.onAnalysisStart?.();
  const { files, graph, order } = await analyzeBuildDirectory(buildDir);
  callbacks?.onAnalysisComplete?.(files.length);

  if (files.length === 0) {
    throw new Error(`No files found in build directory: ${buildDir}`);
  }

  // Step 2: Load manifest and check for version conflicts
  const manifestData = await loadManifestData(MANIFEST_FILENAME);

  if (manifestData && manifestData.existingVersions.includes(version)) {
    const newVersion = suggestNextVersion(version);
    throw new Error(
      `Version "${version}" already exists in local manifest.\n` +
        `  Please use a different version tag (e.g., "${newVersion}" or increment the version number).\n` +
        `  Existing versions: ${manifestData.existingVersions.join(', ')}`
    );
  }

  if (versioningOriginInscription && !dryRun && VERSIONING_ENABLED) {
    try {
      await checkVersionExists(versioningOriginInscription, version);
    } catch (error) {
      throw new Error(`Cannot proceed with deployment: ${formatError(error)}`);
    }
  }

  const previousInscriptions =
    manifestData?.previousInscriptions || new Map<string, InscribedFile>();
  const htmlOriginInscription = manifestData?.htmlOrigin;

  // Step 2.5: Analyze which files can be cached
  let cachedCount = 0;
  const cachedFiles: string[] = [];
  const chunkedFilesInfo: ChunkedFileInfo[] = [];
  const tempUrlMap = new Map<string, string>(); // Temporary map for cache analysis

  for (const filePath of order) {
    const node = graph.get(filePath);
    if (!node) continue;

    const fileRef = node.file;
    const previousInscription = previousInscriptions.get(filePath);

    // Check if we can reuse (same logic as later in wave processing)
    const shouldReuse = canReuseInscription(filePath, fileRef, previousInscription, tempUrlMap);

    if (shouldReuse && previousInscription) {
      cachedCount++;
      cachedFiles.push(filePath);
      tempUrlMap.set(filePath, previousInscription.urlPath);

      // Track chunked files with chunk counts
      if (previousInscription.isChunked && previousInscription.chunks) {
        chunkedFilesInfo.push({
          filename: filePath,
          chunkCount: previousInscription.chunks.length,
          isServiceWorker: false,
          urlPath: previousInscription.urlPath,
        });
      }
    }
  }

  // Check if service worker can be cached (we need to peek ahead)
  // Collect all chunk manifests from cached chunked files
  const cachedChunkManifests: ChunkManifest[] = [];
  const hasChunkedFiles = order.some((filePath) => {
    const node = graph.get(filePath);
    if (!node) return false;
    const isChunked = shouldChunkFile(node.file.fileSize, filePath, DEFAULT_CHUNK_THRESHOLD);

    // If this file is chunked and cached, build its manifest now
    if (isChunked && cachedFiles.includes(filePath)) {
      const prevInscription = previousInscriptions.get(filePath);
      if (prevInscription?.isChunked && prevInscription.chunks) {
        const fileRef = node.file;
        const manifest: ChunkManifest = {
          version: '1.0',
          originalPath: filePath,
          mimeType: fileRef.contentType,
          totalSize: prevInscription.size,
          chunkSize: prevInscription.chunks[0]?.size || 0,
          chunks: prevInscription.chunks.map((c) => ({
            index: c.index,
            txid: c.txid,
            vout: c.vout,
            urlPath: `/content/${c.txid}_${c.vout}`,
            size: c.size,
          })),
        };
        cachedChunkManifests.push(manifest);
      }
    }

    return isChunked;
  });

  if (hasChunkedFiles) {
    // We'll have a service worker - check if it can be cached
    // Generate SW now with cached chunk manifests to calculate hash
    const previousSW = previousInscriptions.get('chunk-reassembly-sw.js');

    if (cachedChunkManifests.length > 0 && previousSW) {
      // Generate SW with cached manifests to check if hash matches
      const swCode = generateChunkReassemblyServiceWorker(cachedChunkManifests, primaryContentUrl);
      const swBuffer = Buffer.from(swCode, 'utf-8');
      const swContentHash = createHash('sha256').update(swBuffer).digest('hex');

      // Check if we can reuse the cached SW
      if (previousSW.contentHash === swContentHash) {
        cachedCount++;
        cachedFiles.push('chunk-reassembly-sw.js');
        chunkedFilesInfo.push({
          filename: 'chunk-reassembly-sw.js',
          chunkCount: 0,
          isServiceWorker: true,
          urlPath: previousSW.urlPath,
        });
      }
    }
  }

  const newCount = files.length + (hasChunkedFiles ? 1 : 0) - cachedCount;
  callbacks?.onCacheAnalysis?.(cachedCount, newCount, cachedFiles, chunkedFilesInfo);

  // Step 3: Deploy empty versioning inscription (FIRST DEPLOYMENT ONLY)
  let finalVersioningOriginInscription = versioningOriginInscription;
  let seedUtxo: Utxo | undefined = undefined; // Change UTXO from versioning inscription to use for first wave
  const txids = new Set<string>();

  if (!versioningOriginInscription && !dryRun && VERSIONING_ENABLED) {
    callbacks?.onInscriptionStart?.(VERSIONING_ORIGIN_TYPE, 1, order.length + 2);
    try {
      const versioningResult = await deployVersioningInscription(
        paymentPk,
        appName || 'ReactApp',
        destinationAddress,
        satsPerKb
      );
      finalVersioningOriginInscription = versioningResult.outpoint;
      seedUtxo = versioningResult.changeUtxo; // Capture change UTXO to avoid indexer timing issues

      const originTxid = finalVersioningOriginInscription.split(OUTPOINT_SEPARATOR)[0];
      txids.add(originTxid);
      callbacks?.onInscriptionComplete?.(
        VERSIONING_ORIGIN_TYPE,
        `${CONTENT_PATH_PREFIX}${finalVersioningOriginInscription}`
      );

      if (seedUtxo) {
        callbacks?.onProgress?.(
          `  âœ“ Change UTXO captured: ${seedUtxo.txid}:${seedUtxo.vout} (${seedUtxo.satoshis} sats)`
        );
      }
    } catch (error) {
      throw new Error(`Failed to deploy versioning inscription: ${formatError(error)}`);
    }
  } else if (dryRun && !versioningOriginInscription) {
    finalVersioningOriginInscription = `${MOCK_VERSIONING_TXID}${OUTPOINT_SEPARATOR}${DEFAULT_INSCRIPTION_VOUT}`;
  }

  // Step 4: Calculate dependency waves
  const { waves } = calculateDependencyWaves(graph);

  // Step 4.5: Separate HTML files from wave processing
  // HTML will be inscribed separately as a 1-sat ordinal chain after all waves and service worker
  const htmlFiles: string[] = [];
  const nonHtmlWaves: string[][] = [];

  for (const wave of waves) {
    const waveHtmlFiles = wave.filter((f) => isIndexHtmlFile(f));
    const waveNonHtmlFiles = wave.filter((f) => !isIndexHtmlFile(f));

    if (waveHtmlFiles.length > 0) {
      htmlFiles.push(...waveHtmlFiles);
    }
    if (waveNonHtmlFiles.length > 0) {
      nonHtmlWaves.push(waveNonHtmlFiles);
    }
  }

  // Waves now contain only non-HTML files
  const reorganizedWaves = nonHtmlWaves;

  callbacks?.onProgress?.(
    `ðŸ“Š Wave structure: ${reorganizedWaves.length} wave(s), HTML inscribed separately`
  );

  // Step 5: Process files wave by wave
  const urlMap = new Map<string, string>();
  // Pre-populate URL map with ALL cached files before any wave processing
  // This ensures that files being rewritten can reference cached files from any wave
  for (const [filePath, previousInscription] of previousInscriptions) {
    if (cachedFiles.includes(filePath)) {
      urlMap.set(filePath, previousInscription.urlPath);
    }
  }
  const inscriptions: InscribedFile[] = [];
  const allChunkManifests: ChunkManifest[] = [];
  // Initialize with cached chunk manifests
  allChunkManifests.push(...cachedChunkManifests);
  let totalCost = 0;
  let totalSize = 0;
  const spentUtxos = new Set<string>(); // Track spent UTXOs across waves (format: txid:vout)
  let serviceWorkerInscription: InscribedFile | undefined;

  const jobContext: WaveJobContext = {
    buildDir,
    destinationAddress,
    versioningOriginInscription: finalVersioningOriginInscription,
    chunkThreshold: DEFAULT_CHUNK_THRESHOLD,
    chunkSize: DEFAULT_CHUNK_SIZE,
    disableChunking: false, // Progressive chunking is always enabled
    serviceWorkerUrl: undefined, // Will be set before HTML wave
  };

  for (let waveIndex = 0; waveIndex < reorganizedWaves.length; waveIndex++) {
    const filesInWave = reorganizedWaves[waveIndex];

    callbacks?.onProgress?.(
      `\nðŸŒŠ Wave ${waveIndex + 1}/${reorganizedWaves.length}: Processing ${filesInWave.length} file(s)...`
    );

    // Filter out cached files
    const filesToInscribe = filesInWave.filter((filePath) => {
      const node = graph.get(filePath);
      if (!node) return true;

      const fileRef = node.file;
      const previousInscription = previousInscriptions.get(filePath);
      const shouldReuse = canReuseInscription(filePath, fileRef, previousInscription, urlMap);

      if (shouldReuse && previousInscription) {
        inscriptions.push({ ...previousInscription, cached: true });
        urlMap.set(filePath, previousInscription.urlPath);
        // Note: Do NOT add cached file txids or sizes to totals - these are from previous deployments

        // If this is a chunked file, note its chunk count
        let chunkCount: number | undefined;
        if (previousInscription.isChunked && previousInscription.chunks) {
          chunkCount = previousInscription.chunks.length;
          // Note: Manifest already added to allChunkManifests during cache analysis
        }

        callbacks?.onInscriptionSkipped?.(filePath, previousInscription.urlPath, chunkCount);
        return false;
      }

      return true;
    });

    if (filesToInscribe.length === 0) {
      callbacks?.onProgress?.(`  âœ“ All files in this wave are cached, skipping...`);
      continue;
    }

    // Prepare jobs for this wave (pass previousInscriptions to skip cached chunked files)
    const jobs = await prepareWaveJobs(
      filesToInscribe,
      graph,
      urlMap,
      jobContext,
      previousInscriptions
    );

    if (jobs.length === 0) {
      continue;
    }

    // Inscribe all jobs in parallel
    const inscriptionResult = await parallelInscribe(
      jobs,
      paymentPk,
      indexer,
      satsPerKb || DEFAULT_SATS_PER_KB,
      dryRun || false,
      DEFAULT_CHUNK_BATCH_SIZE,
      seedUtxo, // Pass seed UTXO for first wave to avoid indexer timing issues
      spentUtxos, // Pass spent UTXOs from previous waves
      callbacks?.onProgress
    );

    // Track split UTXO transaction (in chronological order before inscriptions)
    if (inscriptionResult.splitTxid) {
      txids.add(inscriptionResult.splitTxid);
    }

    // Add cost from this wave
    totalCost += inscriptionResult.totalCost;

    // Clear seed UTXO after first wave (it's been used)
    seedUtxo = undefined;

    // Process results
    // Note: chunkSize passed here is only for metadata - actual chunk sizes are stored in each chunk
    const processed = processWaveResults(inscriptionResult.results, urlMap, jobContext.chunkSize);

    // Add regular files to inscriptions
    for (const inscribedFile of processed.regularFiles) {
      const node = graph.get(inscribedFile.originalPath);
      const fileRef = node?.file;

      const hasDependencies = fileRef?.dependencies && fileRef.dependencies.length > 0;
      const dependencyHash =
        hasDependencies && fileRef
          ? calculateDependencyHash(fileRef.dependencies, urlMap)
          : undefined;

      inscriptions.push({
        ...inscribedFile,
        dependencyHash,
      });

      txids.add(inscribedFile.txid);
      totalSize += inscribedFile.size;
      callbacks?.onInscriptionComplete?.(inscribedFile.originalPath, inscribedFile.urlPath);
    }

    // Handle chunked files
    for (const [filePath, chunkedFileData] of processed.chunkedFiles.entries()) {
      allChunkManifests.push(chunkedFileData.manifest);

      // Get the original file's contentHash from the graph (calculated before chunking)
      const node = graph.get(filePath);
      const fileContentHash = node?.file?.contentHash || '';

      // Build chunks array with full metadata
      const chunksMetadata = chunkedFileData.chunks.map((chunkResult, index) => ({
        index,
        txid: chunkResult.inscription.txid,
        vout: chunkResult.inscription.vout,
        size: chunkResult.inscription.size,
      }));

      // Add the chunked file entry to inscriptions with full metadata
      const chunkedFileInscription: InscribedFile = {
        originalPath: filePath,
        txid: chunkedFileData.manifest.chunks[0]?.txid || '', // Use first chunk's txid as primary
        vout: 0,
        urlPath: `/content/${chunkedFileData.manifest.chunks[0]?.txid}_0`, // Placeholder URL
        size: chunkedFileData.totalSize,
        contentHash: fileContentHash, // Use original file hash, not recalculated from chunks
        isChunked: true,
        chunkCount: chunkedFileData.chunks.length,
        chunks: chunksMetadata,
      };

      inscriptions.push(chunkedFileInscription);

      // Add individual chunk txids
      for (const chunkResult of chunkedFileData.chunks) {
        txids.add(chunkResult.inscription.txid);
      }

      totalSize += chunkedFileData.totalSize;
      callbacks?.onInscriptionComplete?.(filePath, chunkedFileInscription.urlPath);
    }
  }

  // Step 5.5: Inscribe Service Worker (if needed)
  // This happens AFTER all waves are complete but BEFORE HTML inscription
  if (allChunkManifests.length > 0 && !serviceWorkerInscription) {
    callbacks?.onProgress?.(`\nâš™ï¸  Inscribing Service Worker...`);

    // Calculate total files including SW: regular files + versioning files + SW
    const totalFilesIncludingSW = order.length + (VERSIONING_ENABLED ? 2 : 0) + 1;
    const currentFileNumber = inscriptions.length + 1; // Current position

    // Generate service worker code
    const swCode = generateChunkReassemblyServiceWorker(allChunkManifests, primaryContentUrl);
    const swBuffer = Buffer.from(swCode, 'utf-8');

    // Calculate contentHash for caching
    const swContentHash = createHash('sha256').update(swBuffer).digest('hex');

    // Check if we can reuse a previous SW inscription
    const previousSW = previousInscriptions.get('chunk-reassembly-sw.js');
    const canReuseSW = previousSW && previousSW.contentHash === swContentHash;

    if (canReuseSW && previousSW) {
      // Reuse cached service worker
      callbacks?.onInscriptionSkipped?.('chunk-reassembly-sw.js', previousSW.urlPath);

      serviceWorkerInscription = { ...previousSW, cached: true };
      inscriptions.push(serviceWorkerInscription);
      // Note: Do NOT add cached SW txid or size to totals - it's from a previous deployment
      jobContext.serviceWorkerUrl = previousSW.urlPath;

      callbacks?.onProgress?.(
        `  âœ“ Service worker cached (hash: ${swContentHash.slice(0, 16)}...)`
      );
    } else {
      // Inscribe new service worker
      callbacks?.onInscriptionStart?.(
        'chunk-reassembly-sw.js',
        currentFileNumber,
        totalFilesIncludingSW
      );

      const swJobs = [
        {
          id: 'service-worker',
          type: 'bfile' as const,
          filePath: '',
          originalPath: 'chunk-reassembly-sw.js',
          contentType: 'application/javascript',
          content: swBuffer,
          destinationAddress,
        },
      ];

      const swInscriptionResult = await parallelInscribe(
        swJobs,
        paymentPk,
        indexer,
        satsPerKb || DEFAULT_SATS_PER_KB,
        dryRun || false,
        1,
        undefined, // No seed UTXO for SW
        spentUtxos,
        callbacks?.onProgress
      );

      // Track split UTXO transaction for service worker (if any)
      if (swInscriptionResult.splitTxid) {
        txids.add(swInscriptionResult.splitTxid);
      }

      serviceWorkerInscription = {
        ...swInscriptionResult.results[0].inscription,
        contentHash: swContentHash, // Store hash for future caching
      };
      txids.add(serviceWorkerInscription.txid);
      totalCost += swInscriptionResult.totalCost;

      // Add Service Worker to inscriptions immediately (before HTML)
      inscriptions.push(serviceWorkerInscription);
      totalSize += serviceWorkerInscription.size;

      // Update context with SW URL so HTML can reference it
      jobContext.serviceWorkerUrl = serviceWorkerInscription.urlPath;

      callbacks?.onInscriptionComplete?.(
        'chunk-reassembly-sw.js',
        serviceWorkerInscription.urlPath
      );
    }
  }

  // Step 6: Inscribe HTML as 1-sat ordinal chain
  // This happens AFTER all waves and service worker, but BEFORE versioning update
  if (htmlFiles.length === 0) {
    throw new Error('No index.html found in build directory');
  }

  const htmlFilePath = htmlFiles[0]; // Should only be one index.html
  const htmlNode = graph.get(htmlFilePath);
  if (!htmlNode) {
    throw new Error(`Could not find HTML file in graph: ${htmlFilePath}`);
  }

  callbacks?.onProgress?.(`\nðŸŒ Inscribing HTML as 1-sat ordinal...`);

  // Prepare HTML content with all rewrites and script injections
  const htmlContentBuffer = await prepareHtmlContent(
    htmlNode.file,
    htmlFilePath,
    buildDir,
    urlMap,
    finalVersioningOriginInscription,
    jobContext.serviceWorkerUrl
  );

  let entryPoint: InscribedFile;
  let finalHtmlOriginInscription: string;

  if (!htmlOriginInscription && !dryRun) {
    // First deployment: Create initial HTML ordinal
    callbacks?.onInscriptionStart?.('index.html (origin)', inscriptions.length + 1, order.length + 2);

    entryPoint = await createInitialHtmlOrdinal(
      htmlContentBuffer,
      paymentPk,
      indexer,
      destinationAddress,
      satsPerKb || DEFAULT_SATS_PER_KB
    );

    finalHtmlOriginInscription = extractOutpointFromFile(entryPoint);
    txids.add(entryPoint.txid);
    totalCost += 0; // Cost is included in the inscription (no separate split)
    totalSize += entryPoint.size;

    callbacks?.onInscriptionComplete?.('index.html (origin)', entryPoint.urlPath);
  } else if (htmlOriginInscription && !dryRun) {
    // Subsequent deployment: Spend previous HTML ordinal
    callbacks?.onInscriptionStart?.('index.html', inscriptions.length + 1, order.length + 2);

    entryPoint = await updateHtmlOrdinal(
      htmlOriginInscription,
      htmlContentBuffer,
      paymentPk,
      paymentPk, // Use same key as payment key
      indexer,
      destinationAddress,
      satsPerKb || DEFAULT_SATS_PER_KB
    );

    finalHtmlOriginInscription = htmlOriginInscription; // Origin doesn't change
    txids.add(entryPoint.txid);
    totalCost += 0; // Cost is included in the inscription
    totalSize += entryPoint.size;

    callbacks?.onInscriptionComplete?.('index.html', entryPoint.urlPath);
  } else {
    // Dry run: Mock HTML inscription
    const contentHash = createHash('sha256').update(htmlContentBuffer).digest('hex');
    const mockTxid = createHash('sha256')
      .update(`html-${version}-${Date.now()}`)
      .digest('hex');

    entryPoint = {
      originalPath: 'index.html',
      txid: mockTxid,
      vout: 0,
      urlPath: `${CONTENT_PATH_PREFIX}${mockTxid}_0`,
      size: htmlContentBuffer.length,
      contentHash,
    };

    finalHtmlOriginInscription = htmlOriginInscription || `${mockTxid}_0`;
    callbacks?.onInscriptionComplete?.('index.html (dry-run)', entryPoint.urlPath);
  }

  // Add HTML to inscriptions list
  inscriptions.push(entryPoint);

  callbacks?.onProgress?.(`  âœ“ HTML inscribed: ${entryPoint.urlPath}`);

  // Step 7: Update versioning inscription
  let latestVersioningInscription: string | undefined;

  if (finalVersioningOriginInscription && VERSIONING_ENABLED && !dryRun) {
    try {
      const entryPointOutpoint = extractOutpointFromFile(entryPoint);
      latestVersioningInscription = await updateVersioningInscription(
        versioningOriginInscription || finalVersioningOriginInscription,
        paymentPk,
        paymentPk,
        version,
        entryPointOutpoint,
        versionDescription || `Version ${version}`,
        destinationAddress,
        satsPerKb
      );
      const metadataTxid = latestVersioningInscription.split(OUTPOINT_SEPARATOR)[0];
      txids.add(metadataTxid);
      callbacks?.onInscriptionComplete?.(
        VERSIONING_METADATA_TYPE,
        `${CONTENT_PATH_PREFIX}${latestVersioningInscription}`
      );
    } catch (error) {
      console.error('âŒ Failed to update versioning inscription');
      throw error;
    }
  }

  callbacks?.onDeploymentComplete?.(entryPoint.urlPath);

  if (!finalVersioningOriginInscription) {
    throw new Error('Versioning inscription origin was not set');
  }

  return {
    entryPointUrl: entryPoint.urlPath,
    inscriptions,
    totalCost,
    totalSize,
    txids: Array.from(txids),
    versioningOriginInscription: finalVersioningOriginInscription,
    versioningLatestInscription: latestVersioningInscription,
    htmlOriginInscription: finalHtmlOriginInscription,
    version,
    versionDescription,
    buildDir,
    destinationAddress,
    ordinalContentUrl: primaryContentUrl,
  };
}

// Keep existing manifest generation functions
export function generateManifest(result: DeploymentResult): DeploymentManifest {
  const newFiles = result.inscriptions.filter((f) => !f.cached);
  const cachedFilesFull = result.inscriptions.filter((f) => f.cached);

  // Create minimal cached file string array (format: "path::*::txid_vout")
  const cachedFiles = cachedFilesFull.map(
    (f) => `${f.originalPath}${CACHED_FILE_DELIMITER}${f.txid}_${f.vout}`
  );

  return {
    timestamp: new Date().toISOString(),
    entryPoint: result.entryPointUrl,
    files: newFiles,
    cachedFiles,
    totalFiles: newFiles.length,
    totalCost: result.totalCost,
    totalSize: result.totalSize,
    transactions: result.txids,
    latestVersioningInscription: result.versioningLatestInscription,
    htmlOriginInscription: result.htmlOriginInscription,
    version: result.version,
    versionDescription: result.versionDescription,
    buildDir: result.buildDir,
    destinationAddress: result.destinationAddress,
    ordinalContentUrl: result.ordinalContentUrl,
    newFiles: newFiles.length,
    cachedCount: cachedFiles.length,
    newTransactions: result.txids.length,
  };
}

export async function saveManifest(
  manifest: DeploymentManifest,
  outputPath: string
): Promise<void> {
  const json = JSON.stringify(manifest, null, 2);
  await writeFile(outputPath, json, 'utf-8');
}

export async function saveManifestWithHistory(
  manifest: DeploymentManifest,
  outputPath: string,
  originVersioningInscription: string,
  originHtmlInscription?: string
): Promise<DeploymentManifestHistory> {
  let history: DeploymentManifestHistory;

  if (existsSync(outputPath)) {
    try {
      const existing = await readFile(outputPath, 'utf-8');
      const parsed = JSON.parse(existing);

      if ('manifestVersion' in parsed && 'deployments' in parsed) {
        history = parsed as DeploymentManifestHistory;
        history.deployments.push(manifest);
        history.totalDeployments = history.deployments.length;
        if (!history.originVersioningInscription) {
          history.originVersioningInscription = originVersioningInscription;
        }
        if (!history.originHtmlInscription && originHtmlInscription) {
          history.originHtmlInscription = originHtmlInscription;
        }
      } else if ('timestamp' in parsed && 'entryPoint' in parsed) {
        const oldManifest = parsed as DeploymentManifest;

        const { originVersioningInscription, originHtmlInscription: oldHtmlOrigin, ...cleanedOldManifest } =
          oldManifest as DeploymentManifest & { originVersioningInscription?: string; originHtmlInscription?: string };
        history = {
          manifestVersion: MANIFEST_VERSION,
          originVersioningInscription: originVersioningInscription,
          originHtmlInscription: originHtmlInscription || oldHtmlOrigin,
          totalDeployments: 2,
          deployments: [cleanedOldManifest, manifest],
        };
      } else {
        history = createNewHistory(manifest, originVersioningInscription, originHtmlInscription);
      }
    } catch {
      history = createNewHistory(manifest, originVersioningInscription, originHtmlInscription);
    }
  } else {
    history = createNewHistory(manifest, originVersioningInscription, originHtmlInscription);
  }

  const json = JSON.stringify(history, null, 2);
  await writeFile(outputPath, json, 'utf-8');

  return history;
}

function createNewHistory(
  manifest: DeploymentManifest,
  originVersioningInscription: string,
  originHtmlInscription?: string
): DeploymentManifestHistory {
  return {
    manifestVersion: MANIFEST_VERSION,
    originVersioningInscription: originVersioningInscription,
    originHtmlInscription,
    totalDeployments: 1,
    deployments: [manifest],
  };
}
